{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download additional dataset from Roboflow to merge with exisiting training dataset.\n",
    "https://public.roboflow.com/object-detection/self-driving-car/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"Uf5F9mzBsL27ozIYiWvW\")\n",
    "project = rf.workspace(\"roboflow-gw7yv\").project(\"self-driving-car\")\n",
    "version = project.version(3)\n",
    "dataset = version.download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual copy to Masterfile folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def adjust_label_indices(label_dir, adjustment):\n",
    "    for label_file in os.listdir(label_dir):\n",
    "        path = os.path.join(label_dir, label_file)\n",
    "        with open(path, 'r+') as file:\n",
    "            lines = file.readlines()\n",
    "            file.seek(0)\n",
    "            file.truncate()  # Clear existing content\n",
    "            for line in lines:\n",
    "                parts = line.split()\n",
    "                parts[0] = str(int(parts[0]) + adjustment)  # Adjust the class index\n",
    "                file.write(\" \".join(parts) + \"\\n\")\n",
    "\n",
    "# Adjust labels for the self-driving car dataset (class indices start at 7)\n",
    "adjust_label_indices('/home/mangguai/capstone/code/Master_Dataset/export/labels', 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_classes(label_dir):\n",
    "    class_counts = defaultdict(int)  # Dictionary to store counts of each class index\n",
    "    \n",
    "    for label_file in os.listdir(label_dir):\n",
    "        path = os.path.join(label_dir, label_file)\n",
    "        with open(path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                class_index = line.split()[0]  # Get the class index from each line\n",
    "                class_counts[int(class_index)] += 1  # Increment count for this class\n",
    "\n",
    "    return class_counts\n",
    "\n",
    "# Example usage\n",
    "label_dir = '/home/mangguai/capstone/code/Master_Dataset/export/labels'\n",
    "class_counts = count_classes(label_dir)\n",
    "print(\"Class counts:\")\n",
    "for class_index in sorted(class_counts):\n",
    "    print(f\"Class {class_index}: {class_counts[class_index]} images\")\n",
    "print(\"Number of unique classes:\", len(class_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_label_files_with_classes(label_dir, class_range):\n",
    "    class_files = defaultdict(list)  # Dictionary to store lists of files for each class index\n",
    "\n",
    "    for label_file in os.listdir(label_dir):\n",
    "        path = os.path.join(label_dir, label_file)\n",
    "        with open(path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            found_classes = set()  # To avoid adding the same file multiple times for the same class\n",
    "            for line in lines:\n",
    "                class_index = int(line.split()[0])  # Get the class index from each line\n",
    "                if class_index in class_range and class_index not in found_classes:\n",
    "                    if len(class_files[class_index]) < 5:  # Ensure no more than 5 files are stored\n",
    "                        class_files[class_index].append(label_file)\n",
    "                    found_classes.add(class_index)\n",
    "\n",
    "    return class_files\n",
    "\n",
    "# Define the class range you are interested in\n",
    "class_range = set(range(7, 18))  # From 7 to 17\n",
    "\n",
    "# Example usage\n",
    "label_dir = '/home/mangguai/capstone/code/Master_Dataset/export/labels'\n",
    "class_files = find_label_files_with_classes(label_dir, class_range)\n",
    "\n",
    "# Printing the results with newline for each file name\n",
    "for class_index in class_range:\n",
    "    if class_index in class_files:  # Check if there are any files for the class\n",
    "        files = \"\\n\".join(class_files[class_index])  # Join files with newline character\n",
    "        print(f\"Class {class_index} has the following label files:\\n{files}\\n\")\n",
    "    else:\n",
    "        print(f\"Class {class_index} has no label files.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Based on your description, you want to merge specific traffic light classes with their corresponding \"left\" direction classes and then reassign the indices to reduce the total number of classes. Your mapping appears mostly correct according to your specifications, but let's review and confirm the reassignments for clarity:\n",
    "\n",
    "Biker (7) remains 7.\n",
    "Car (8) remains 8.\n",
    "Pedestrians (9) remains 9.\n",
    "Traffic Light (10) remains 10.\n",
    "Traffic Light-Green (11) and Traffic Light-GreenLeft (12) both become 11.\n",
    "Traffic Light-Red (13) and Traffic Light-RedLeft (14) both become 12.\n",
    "Traffic Light-Yellow (15) and Traffic Light-YellowLeft (16) both become 13.\n",
    "Truck (17) becomes 14.\n",
    "The assignment seems correct based on the pattern you are following: merging the standard and \"left\" variations of traffic lights into a single class and subsequently renumbering the remaining classes.\n",
    "\n",
    "Here's the simplified class structure based on your description:\n",
    "\n",
    "7: Biker\n",
    "8: Car\n",
    "9: Pedestrians\n",
    "10: Traffic Light\n",
    "11: Traffic Light-Green (merged with Traffic Light-GreenLeft)\n",
    "12: Traffic Light-Red (merged with Traffic Light-RedLeft)\n",
    "13: Traffic Light-Yellow (merged with Traffic Light-YellowLeft)\n",
    "14: Truck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def adjust_classes(label_dir, mapping):\n",
    "    for label_file in os.listdir(label_dir):\n",
    "        path = os.path.join(label_dir, label_file)\n",
    "        with open(path, 'r+') as file:\n",
    "            lines = file.readlines()\n",
    "            file.seek(0)\n",
    "            file.truncate()  # Clear the file\n",
    "            for line in lines:\n",
    "                parts = line.split()\n",
    "                class_index = int(parts[0])\n",
    "                if class_index in mapping:\n",
    "                    parts[0] = str(mapping[class_index])  # Update class index based on mapping\n",
    "                file.write(\" \".join(parts) + \"\\n\")\n",
    "\n",
    "# Define your class mapping\n",
    "mapping = {\n",
    "    7: 7,\n",
    "    8: 8,\n",
    "    9: 9,\n",
    "    10: 10,\n",
    "    11: 11,\n",
    "    12: 11,\n",
    "    13: 12,\n",
    "    14: 12,\n",
    "    15: 13,\n",
    "    16: 13,\n",
    "    17: 14\n",
    "}\n",
    "\n",
    "# Example usage\n",
    "label_dir = '/home/mangguai/capstone/code/Master_Dataset/export/labels'\n",
    "adjust_classes(label_dir, mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recheck the class total numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_classes(label_dir):\n",
    "    class_counts = defaultdict(int)  # Dictionary to store counts of each class index\n",
    "    \n",
    "    for label_file in os.listdir(label_dir):\n",
    "        path = os.path.join(label_dir, label_file)\n",
    "        with open(path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                class_index = line.split()[0]  # Get the class index from each line\n",
    "                class_counts[int(class_index)] += 1  # Increment count for this class\n",
    "\n",
    "    return class_counts\n",
    "\n",
    "# Example usage\n",
    "label_dir = '/home/mangguai/capstone/code/Master_Dataset/export/labels'\n",
    "class_counts = count_classes(label_dir)\n",
    "print(\"Class counts:\")\n",
    "for class_index in sorted(class_counts):\n",
    "    print(f\"Class {class_index}: {class_counts[class_index]} images\")\n",
    "print(\"Number of unique classes:\", len(class_counts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data now ready to merge. need to edit yaml file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now the image and label ready to split\n",
    "\n",
    "can you help me split into test/train/valid?\n",
    "can access to label file to see the total number of different class, help me split 70% for training, 20% for validation and 10% for testing...\n",
    "understand that some image have multiple labels, please help me prioritize the lowest count first to split with that\n",
    "\n",
    "the file location as below:\n",
    "File to split out from: /home/mangguai/capstone/code/Master_Dataset/export (inside there is images and labels folder)\n",
    "Split to: /home/mangguai/capstone/code/Master_Dataset/test (inside there is images and labels folder)\n",
    "/home/mangguai/capstone/code/Master_Dataset/train (inside there is images and labels folder)\n",
    "/home/mangguai/capstone/code/Master_Dataset/valid (inside there is images and labels folder)\n",
    "\n",
    "reminder: please split both image and label file with the same name under same test/train/valid folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To split the dataset into training, validation, and testing sets while considering class frequencies to ensure a balanced split, you can use a Python script. This process involves counting occurrences of each class in the labels, prioritizing splits based on class rarity, and then distributing images and their corresponding labels into the designated directories. Here's a step-by-step Python script to help you achieve this:\n",
    "\n",
    "Count class frequencies in the labels.\n",
    "Assign images to splits based on class rarity, aiming for a distribution of 70% training, 20% validation, and 10% testing.\n",
    "Move files to their respective directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Points of the Script:\n",
    "Counting Classes and Images: It calculates how often each class appears and which classes each image contains.\n",
    "Sorting by Rarity: Images are sorted based on the rarity of the classes they contain. This helps in distributing less frequent classes more evenly across the splits.\n",
    "Splitting the Data: Uses train_test_split from sklearn to split the dataset into the desired ratios.\n",
    "Moving Files: Corresponding image and label files are moved to their new directories.\n",
    "Considerations:\n",
    "File Types: This script assumes label files are .txt and image files are .jpg. Adjust the extensions if your dataset uses different formats.\n",
    "Random Seed: A random seed (42) is used for reproducibility in splits.\n",
    "Directory Structure: Ensure that the directories exist before running the script or add code to create them if they do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def count_classes(label_dir):\n",
    "    class_counts = defaultdict(int)\n",
    "    image_classes = defaultdict(set)\n",
    "    \n",
    "    for label_file in os.listdir(label_dir):\n",
    "        path = os.path.join(label_dir, label_file)\n",
    "        with open(path, 'r') as file:\n",
    "            for line in file:\n",
    "                class_index = line.split()[0]\n",
    "                class_counts[int(class_index)] += 1\n",
    "                image_classes[label_file].add(int(class_index))\n",
    "    return class_counts, image_classes\n",
    "\n",
    "def split_datasets(base_dir, output_dirs, split_ratio=(0.7, 0.2, 0.1)):\n",
    "    label_dir = os.path.join(base_dir, 'labels')\n",
    "    image_dir = os.path.join(base_dir, 'images')\n",
    "    \n",
    "    class_counts, image_classes = count_classes(label_dir)\n",
    "    \n",
    "    # Sort images by the rarity of their classes\n",
    "    sorted_images = sorted(image_classes.keys(), key=lambda x: min(class_counts[i] for i in image_classes[x]))\n",
    "    \n",
    "    # Split images into training, validation, and testing\n",
    "    train_val, test = train_test_split(sorted_images, test_size=split_ratio[2], random_state=42)\n",
    "    train, val = train_test_split(train_val, test_size=split_ratio[1] / (split_ratio[0] + split_ratio[1]), random_state=42)\n",
    "    \n",
    "    # Function to move files\n",
    "    def move_files(files, dest_dir):\n",
    "        for file_name in files:\n",
    "            # Move label file\n",
    "            shutil.move(os.path.join(label_dir, file_name), os.path.join(dest_dir, 'labels', file_name))\n",
    "            # Move corresponding image file\n",
    "            image_file_name = file_name.replace('.txt', '.jpg')  # Assuming image files are .jpg\n",
    "            shutil.move(os.path.join(image_dir, image_file_name), os.path.join(dest_dir, 'images', image_file_name))\n",
    "    \n",
    "    # Move files to respective directories\n",
    "    move_files(train, os.path.join(output_dirs['train']))\n",
    "    move_files(val, os.path.join(output_dirs['valid']))\n",
    "    move_files(test, os.path.join(output_dirs['test']))\n",
    "\n",
    "# Example usage\n",
    "base_dir = '/home/mangguai/capstone/code/Master_Dataset/export'\n",
    "output_dirs = {\n",
    "    'train': '/home/mangguai/capstone/code/Master_Dataset/train',\n",
    "    'valid': '/home/mangguai/capstone/code/Master_Dataset/valid',\n",
    "    'test': '/home/mangguai/capstone/code/Master_Dataset/test'\n",
    "}\n",
    "split_datasets(base_dir, output_dirs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are 3500 images leftover due to no labelling\n",
    "manual update yaml file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset are ready to be train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the classes and their corresponding indices\n",
    "class_names = [\n",
    "    'curveleft_sign',           # Class 0\n",
    "    'curveright_sign',          # Class 1\n",
    "    'mandatorystop_sign',       # Class 2\n",
    "    'noentry_sign',             # Class 3\n",
    "    'nojaywalking_sign',        # Class 4\n",
    "    'pedestriancrossing_sign',  # Class 5\n",
    "    'zebracrossing_sign',       # Class 6\n",
    "    'biker',                    # Class 7\n",
    "    'car',                      # Class 8\n",
    "    'pedestrian',               # Class 9\n",
    "    'trafficlight',             # Class 10\n",
    "    'trafficlight_green',       # Class 11\n",
    "    'trafficlight_red',         # Class 12\n",
    "    'trafficlight_yellow',      # Class 13\n",
    "    'truck'                     # Class 14\n",
    "]\n",
    "class_indices = {name: index for index, name in enumerate(class_names)}\n",
    "\n",
    "# Image dimensions\n",
    "img_width, img_height = 640, 640\n",
    "\n",
    "# Path to the folder containing the annotation files\n",
    "folder_path = 'Master_Dataset/train/labels'\n",
    "output_folder_path = 'Master_Dataset/train/modified_labels'  # New output folder\n",
    "\n",
    "os.makedirs(output_folder_path, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "# Function to normalize coordinates\n",
    "def normalize_coordinates(x, y, w, h, img_width, img_height):\n",
    "    dw = 1.0 / img_width\n",
    "    dh = 1.0 / img_height\n",
    "    x_center = (x + w / 2.0) * dw\n",
    "    y_center = (y + h / 2.0) * dh\n",
    "    width = w * dw\n",
    "    height = h * dh\n",
    "    return x_center, y_center, width, height\n",
    "\n",
    "# Process each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".txt\"):  # assuming the annotation files are .txt files\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        output_file_path = os.path.join(output_folder_path, filename)  # Output file path\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        new_lines = []\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 5:  # Ensure this is a YOLO format line\n",
    "                class_index, x, y, w, h = map(float, parts)\n",
    "                # Normalize coordinates\n",
    "                x_center, y_center, width, height = normalize_coordinates(x, y, w, h, img_width, img_height)\n",
    "                new_line = f\"{int(class_index)} {x_center} {y_center} {width} {height}\"\n",
    "                new_lines.append(new_line)\n",
    "\n",
    "        # Write the converted lines to a new file in the output directory\n",
    "        with open(output_file_path, 'w') as file:\n",
    "            file.write('\\n'.join(new_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.10 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.196 ðŸš€ Python-3.11.8 torch-2.2.2 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/home/mangguai/capstone/code/Master_Dataset/data.yaml, epochs=20, patience=50, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train4\n",
      "Overriding model.yaml nc=80 with nc=15\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2121853  ultralytics.nn.modules.head.Detect           [15, [128, 256, 512]]         \n",
      "Model summary: 225 layers, 11141405 parameters, 11141389 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train4', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/mangguai/capstone/code/Master_Dataset/train/labels.cache... 20769 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20769/20769 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/mangguai/capstone/code/Master_Dataset/train/images/1478021875081281646_jpg.rf.e9552980cf8c6fef4aa02cb84c6364f5.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/mangguai/capstone/code/Master_Dataset/train/images/1478898145212453716_jpg.rf.6a92d7d7dd523160c990c4e4375bcea9.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/mangguai/capstone/code/Master_Dataset/train/images/1478898145212453716_jpg.rf.nCaFkPk4AFMjTQAM4RTJ.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/mangguai/capstone/code/Master_Dataset/valid/labels.cache... 5430 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5430/5430 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/mangguai/capstone/code/Master_Dataset/valid/images/1478897760163798179_jpg.rf.98623be50b02ff17d58f89fddf7a0c6c.jpg: 1 duplicate labels removed\n",
      "Plotting labels to runs/detect/train4/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000526, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train4\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/20      2.46G      1.459      1.292      1.168          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:50<00:00,  7.40it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:52<00:00,  6.51it/s]\n",
      "                   all       5430      39476      0.636      0.649      0.652      0.449\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/20      2.58G      1.411      0.937      1.156          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:36<00:00,  7.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:58<00:00,  5.86it/s]\n",
      "                   all       5430      39476      0.649      0.713      0.683      0.425\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/20      2.58G      1.397     0.9078       1.15          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:42<00:00,  7.57it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:51<00:00,  6.57it/s]\n",
      "                   all       5430      39476      0.737      0.725      0.704        0.5\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/20      2.57G      1.381     0.8814      1.142         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:43<00:00,  7.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:54<00:00,  6.18it/s]\n",
      "                   all       5430      39476      0.708      0.736      0.725      0.522\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/20      2.56G      1.361     0.8521       1.13         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:34<00:00,  7.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:56<00:00,  6.01it/s]\n",
      "                   all       5430      39476      0.759      0.733      0.732      0.515\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/20      2.57G      1.338      0.828      1.122          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:43<00:00,  7.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:55<00:00,  6.08it/s]\n",
      "                   all       5430      39476      0.763      0.759       0.75       0.53\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/20      2.56G      1.327     0.8092      1.115          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:43<00:00,  7.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:48<00:00,  6.99it/s]\n",
      "                   all       5430      39476      0.787       0.76      0.754      0.529\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/20      2.57G      1.307      0.789      1.105          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:43<00:00,  7.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:54<00:00,  6.23it/s]\n",
      "                   all       5430      39476      0.787      0.772      0.783      0.568\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/20      2.56G      1.288     0.7684      1.099          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:34<00:00,  7.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:57<00:00,  5.88it/s]\n",
      "                   all       5430      39476      0.784      0.772      0.767      0.551\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/20      2.57G      1.275     0.7531      1.089          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:41<00:00,  7.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:58<00:00,  5.81it/s]\n",
      "                   all       5430      39476      0.803      0.773      0.772      0.558\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/20      2.56G      1.315     0.7311      1.104         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:34<00:00,  7.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:57<00:00,  5.88it/s]\n",
      "                   all       5430      39476      0.805      0.783      0.783      0.576\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/20      2.56G      1.294     0.7073      1.095          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:42<00:00,  7.57it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:56<00:00,  6.07it/s]\n",
      "                   all       5430      39476      0.819      0.778      0.799      0.586\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/20      2.55G      1.277     0.6903      1.086          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:42<00:00,  7.58it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:58<00:00,  5.77it/s]\n",
      "                   all       5430      39476       0.81      0.792      0.794      0.584\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/20      2.56G      1.258     0.6725      1.079          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:38<00:00,  7.67it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:57<00:00,  5.90it/s]\n",
      "                   all       5430      39476      0.817      0.794      0.826      0.619\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/20      2.56G      1.239     0.6567      1.073          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:30<00:00,  7.85it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:52<00:00,  6.50it/s]\n",
      "                   all       5430      39476      0.831      0.792      0.825      0.615\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/20      2.56G      1.222     0.6426      1.067          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:41<00:00,  7.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:56<00:00,  6.04it/s]\n",
      "                   all       5430      39476      0.822      0.809       0.82      0.614\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/20      2.55G      1.204     0.6264      1.058          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:35<00:00,  7.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:52<00:00,  6.47it/s]\n",
      "                   all       5430      39476      0.825      0.812      0.839      0.635\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/20      2.56G      1.191     0.6127      1.051         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:27<00:00,  7.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:53<00:00,  6.30it/s]\n",
      "                   all       5430      39476      0.821      0.802      0.851      0.652\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/20      2.56G      1.171      0.602      1.042          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [06:43<00:00,  6.43it/s]  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:56<00:00,  5.96it/s]\n",
      "                   all       5430      39476      0.847      0.799       0.86      0.656\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/20      2.56G      1.156     0.5907      1.037          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:29<00:00,  7.89it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:53<00:00,  6.33it/s]\n",
      "                   all       5430      39476      0.845      0.809      0.862       0.66\n",
      "\n",
      "20 epochs completed in 2.214 hours.\n",
      "Optimizer stripped from runs/detect/train4/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from runs/detect/train4/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating runs/detect/train4/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.196 ðŸš€ Python-3.11.8 torch-2.2.2 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "Model summary (fused): 168 layers, 11131389 parameters, 0 gradients, 28.5 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [01:03<00:00,  5.35it/s]\n",
      "                   all       5430      39476      0.845      0.809      0.862       0.66\n",
      "     nojaywalking_sign       5430          6      0.309          1      0.894      0.838\n",
      "    mandatorystop_sign       5430         14      0.698          1      0.933      0.881\n",
      "       curveright_sign       5430         19      0.938          1      0.993      0.918\n",
      "    zebracrossing_sign       5430         26      0.934      0.962      0.971      0.865\n",
      "          noentry_sign       5430         47      0.983      0.936      0.979      0.945\n",
      "pedestriancrossing_sign       5430         33      0.963      0.786      0.899      0.825\n",
      "        curveleft_sign       5430         33      0.982          1      0.995       0.93\n",
      "                 biker       5430        765      0.792      0.633      0.702      0.399\n",
      "                   car       5430      25812      0.891      0.805      0.865       0.59\n",
      "            pedestrian       5430       4114       0.84      0.563      0.688      0.359\n",
      "          trafficlight       5430       1038      0.859      0.808      0.868      0.556\n",
      "    trafficlight_green       5430       2283      0.855      0.651      0.748      0.365\n",
      "      trafficlight_red       5430       3718      0.924       0.73      0.842      0.506\n",
      "   trafficlight_yellow       5430        115      0.872      0.443      0.674      0.293\n",
      "                 truck       5430       1453      0.834      0.822      0.879      0.626\n",
      "Speed: 0.2ms preprocess, 3.4ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train4\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='yolo train model=yolov8s.pt data=/home/mangguai/capstone/code/Master_Dataset/data.yaml epochs=20 imgsz=640 batch=8', returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Command to run\n",
    "command = \"yolo train model=yolov8s.pt data=/home/mangguai/capstone/code/Master_Dataset/data.yaml epochs=20 imgsz=640 batch=8\"\n",
    "\n",
    "# Execute the command\n",
    "subprocess.run(command, shell=True)\n",
    "\n",
    "#134min for 20 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.10 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.196 ðŸš€ Python-3.11.8 torch-2.2.2 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/home/mangguai/capstone/code/Master_Dataset/data.yaml, epochs=25, patience=50, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train5\n",
      "Overriding model.yaml nc=80 with nc=15\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2121853  ultralytics.nn.modules.head.Detect           [15, [128, 256, 512]]         \n",
      "Model summary: 225 layers, 11141405 parameters, 11141389 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train5', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/mangguai/capstone/code/Master_Dataset/train/labels.cache... 20769 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20769/20769 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/mangguai/capstone/code/Master_Dataset/train/images/1478021875081281646_jpg.rf.e9552980cf8c6fef4aa02cb84c6364f5.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/mangguai/capstone/code/Master_Dataset/train/images/1478898145212453716_jpg.rf.6a92d7d7dd523160c990c4e4375bcea9.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/mangguai/capstone/code/Master_Dataset/train/images/1478898145212453716_jpg.rf.nCaFkPk4AFMjTQAM4RTJ.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/mangguai/capstone/code/Master_Dataset/valid/labels.cache... 5430 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5430/5430 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/mangguai/capstone/code/Master_Dataset/valid/images/1478897760163798179_jpg.rf.98623be50b02ff17d58f89fddf7a0c6c.jpg: 1 duplicate labels removed\n",
      "Plotting labels to runs/detect/train5/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000526, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train5\u001b[0m\n",
      "Starting training for 25 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/25      2.46G      1.459      1.292      1.168          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:55<00:00,  7.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:53<00:00,  6.36it/s]\n",
      "                   all       5430      39476      0.636      0.649      0.652      0.449\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/25      2.58G      1.412     0.9391      1.156          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:42<00:00,  7.58it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [01:03<00:00,  5.35it/s]\n",
      "                   all       5430      39476      0.604      0.721       0.67      0.461\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/25      2.57G        1.4     0.9139       1.15          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:37<00:00,  7.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:56<00:00,  6.07it/s]\n",
      "                   all       5430      39476      0.685      0.729      0.714      0.519\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/25      2.57G      1.383     0.8847      1.141         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:38<00:00,  7.66it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:52<00:00,  6.44it/s]\n",
      "                   all       5430      39476      0.751      0.738      0.717      0.509\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/25      2.56G      1.363     0.8556      1.135         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:35<00:00,  7.75it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:58<00:00,  5.79it/s]\n",
      "                   all       5430      39476      0.784        0.7      0.747      0.533\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/25      2.57G      1.344     0.8335      1.126          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:33<00:00,  7.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:59<00:00,  5.71it/s]\n",
      "                   all       5430      39476      0.763      0.754      0.741      0.523\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/25      2.57G       1.33     0.8146      1.118          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:44<00:00,  7.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:56<00:00,  6.00it/s]\n",
      "                   all       5430      39476       0.79      0.754      0.741      0.536\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/25      2.57G      1.313     0.7945      1.107          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:40<00:00,  7.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [01:02<00:00,  5.44it/s]\n",
      "                   all       5430      39476       0.79      0.762      0.763      0.543\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/25      2.57G      1.296     0.7761      1.102          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:32<00:00,  7.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:57<00:00,  5.89it/s]\n",
      "                   all       5430      39476      0.792      0.769      0.766      0.558\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/25      2.57G      1.282     0.7606      1.093          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:32<00:00,  7.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:55<00:00,  6.10it/s]\n",
      "                   all       5430      39476      0.784       0.77      0.763      0.545\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/25      2.56G      1.273     0.7494      1.091          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:31<00:00,  7.85it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [01:00<00:00,  5.66it/s]\n",
      "                   all       5430      39476      0.819      0.766      0.787       0.58\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/25      2.56G       1.26     0.7377      1.085          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:32<00:00,  7.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:57<00:00,  5.95it/s]\n",
      "                   all       5430      39476      0.798      0.782      0.788      0.581\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/25      2.55G      1.249     0.7221      1.078         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:28<00:00,  7.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:53<00:00,  6.38it/s]\n",
      "                   all       5430      39476      0.796      0.789      0.786      0.574\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/25      2.57G      1.236     0.7101      1.072          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:12<00:00,  8.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:51<00:00,  6.66it/s]\n",
      "                   all       5430      39476      0.794      0.798      0.798      0.588\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/25      2.56G      1.227     0.7007       1.07         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:35<00:00,  7.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:58<00:00,  5.78it/s]\n",
      "                   all       5430      39476        0.8      0.795      0.807      0.599\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/25      2.57G      1.262     0.6777      1.082          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:43<00:00,  7.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [01:01<00:00,  5.51it/s]\n",
      "                   all       5430      39476      0.811      0.793      0.801      0.593\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/25      2.56G      1.241     0.6573      1.074          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:35<00:00,  7.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:56<00:00,  6.02it/s]\n",
      "                   all       5430      39476      0.807      0.794      0.803      0.599\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/25      2.57G      1.226     0.6432      1.065         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:43<00:00,  7.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:58<00:00,  5.82it/s]\n",
      "                   all       5430      39476      0.816      0.808      0.815      0.612\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/25      2.56G      1.211     0.6336      1.058          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:38<00:00,  7.67it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:58<00:00,  5.82it/s]\n",
      "                   all       5430      39476      0.819      0.809      0.822      0.616\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/25      2.57G      1.195     0.6197      1.053          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:35<00:00,  7.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:57<00:00,  5.91it/s]\n",
      "                   all       5430      39476      0.832        0.8      0.817      0.614\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/25      2.54G       1.18      0.607      1.046          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:32<00:00,  7.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:59<00:00,  5.68it/s]\n",
      "                   all       5430      39476       0.83      0.814      0.825      0.623\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/25      2.56G      1.167     0.5959      1.043          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:26<00:00,  7.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:56<00:00,  5.97it/s]\n",
      "                   all       5430      39476      0.857      0.789      0.842      0.639\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/25      2.55G      1.152     0.5855      1.036          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:20<00:00,  8.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:52<00:00,  6.47it/s]\n",
      "                   all       5430      39476      0.823      0.819      0.837      0.636\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/25      2.56G      1.141     0.5761      1.031         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:24<00:00,  8.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:55<00:00,  6.07it/s]\n",
      "                   all       5430      39476      0.843      0.806      0.839       0.64\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/25      2.55G      1.127     0.5677      1.026         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2597/2597 [05:26<00:00,  7.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:56<00:00,  6.02it/s]\n",
      "                   all       5430      39476      0.855      0.805      0.867      0.668\n",
      "\n",
      "25 epochs completed in 2.729 hours.\n",
      "Optimizer stripped from runs/detect/train5/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from runs/detect/train5/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating runs/detect/train5/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.196 ðŸš€ Python-3.11.8 torch-2.2.2 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "Model summary (fused): 168 layers, 11131389 parameters, 0 gradients, 28.5 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [01:08<00:00,  4.93it/s]\n",
      "                   all       5430      39476      0.857      0.804      0.867      0.669\n",
      "        curveleft_sign       5430          6      0.346          1      0.866       0.85\n",
      "       curveright_sign       5430         14      0.791          1      0.967      0.911\n",
      "    mandatorystop_sign       5430         19      0.932      0.947      0.963        0.9\n",
      "          noentry_sign       5430         26      0.942      0.962      0.967      0.842\n",
      "     nojaywalking_sign       5430         47      0.984      0.936      0.972      0.932\n",
      "pedestriancrossing_sign       5430         33      0.849      0.854       0.94      0.878\n",
      "    zebracrossing_sign       5430         33      0.984          1      0.995      0.931\n",
      "                 biker       5430        765      0.829      0.604      0.719      0.415\n",
      "                   car       5430      25812      0.902      0.802      0.868      0.598\n",
      "            pedestrian       5430       4114      0.865      0.545      0.702      0.368\n",
      "          trafficlight       5430       1038      0.866      0.801      0.874      0.563\n",
      "    trafficlight_green       5430       2283       0.87      0.631      0.759       0.38\n",
      "      trafficlight_red       5430       3718      0.931      0.733      0.845      0.514\n",
      "   trafficlight_yellow       5430        115      0.873      0.443       0.68      0.313\n",
      "                 truck       5430       1453       0.89      0.803      0.889      0.641\n",
      "Speed: 0.2ms preprocess, 3.4ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train5\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='yolo train model=yolov8s.pt data=/home/mangguai/capstone/code/Master_Dataset/data.yaml epochs=25 imgsz=640 batch=8', returncode=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Command to run\n",
    "command = \"yolo train model=yolov8s.pt data=/home/mangguai/capstone/code/Master_Dataset/data.yaml epochs=25 imgsz=640 batch=8\"\n",
    "\n",
    "# Execute the command\n",
    "subprocess.run(command, shell=True)\n",
    "\n",
    "#165min for 25 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.196 ðŸš€ Python-3.11.8 torch-2.2.2 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "Model summary (fused): 168 layers, 11131389 parameters, 0 gradients, 28.5 GFLOPs\n",
      "\n",
      "image 1/1 /home/mangguai/capstone/data/greenlight.jpg: 416x640 1 biker, 2 pedestrians, 92.6ms\n",
      "Speed: 3.1ms preprocess, 92.6ms inference, 99.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict9\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "#before running this command, make sure you are in the correct directory, check with pwd\n",
    "!yolo task=detect mode=predict model=/home/mangguai/capstone/code/runs/detect/train5/weights/best.pt conf=0.25 source=/home/mangguai/capstone/data/greenlight.jpg\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
