{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Conceptual Neural Networks Lab\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In class, you've seen how to build a neural network in Keras. This lab is intended to help you understand exactly how a neural network is built and understand the math behind it. We will walk through the process of building a *very* simple neural network by hand.\n",
    "\n",
    "Let's start at the beginning. In general, a forward pass through a neural network works like this:\n",
    "1. Take our inputs and multiply each input by a weight.\n",
    "2. Add all of the results from step 1 together.\n",
    "3. Add a bias to the result from step 2.\n",
    "4. Pass the value from step 3 through an activation function.\n",
    "5. This is now passed to the next layer.\n",
    "\n",
    "![](./images/perceptron.jpeg)\n",
    "\n",
    "([*image source*](https://www.kdnuggets.com/2016/10/beginners-guide-neural-networks-python-scikit-learn.html))\n",
    "\n",
    "Let's code a simple one-layer neural network together!\n",
    "\n",
    "**The ONLY Python libraries you will need/can use in this lab are numpy and matplotlib!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy and matplotlib - this is all you will need for this lab!\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "# Set a random seed\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1:** Let's write a function called `update` that takes an array of our inputs, multiplies these by their weights, and sums them together with a bias. We will start with random initialization of our weights and bias. For simplicity, the network we will build will just have 2 inputs.\n",
    "\n",
    "For example,\n",
    "```python\n",
    "inputs = np.array([12, 4])\n",
    "weights = np.array([1, 2])\n",
    "bias = 1\n",
    "\n",
    "update(inputs, weights, bias)\n",
    "```\n",
    "Should return:\n",
    "```python\n",
    "21\n",
    "```\n",
    "\n",
    "Since: $ (12*1) + (4*2) + 1 = 21 $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs and initial weights/bias:\n",
    "\n",
    "inputs = np.array([12, 4])\n",
    "weights = np.array([1, 2])\n",
    "bias = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "\n",
    "def update(inputs, weights, bias):\n",
    "    return np.dot(inputs, weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update(inputs, weights, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2:** Now, let's code our activation function. For best performance, we often use ReLU as our activation function in hidden layers. Graphically, this looks like:\n",
    "\n",
    "![](./images/relu.png)\n",
    "\n",
    "([*image source*](https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7))\n",
    "\n",
    "And can be written as:\n",
    "\n",
    "$$\n",
    "f(x) = \\left\\{\\begin{matrix}\n",
    "0 \\text{  for  } x<0\\\\\n",
    "x \\text{  for  } x\\geq 0\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "Write a function called `relu` that takes in a value and returns the value after it has been passed through the ReLU function.\n",
    "\n",
    "For example,\n",
    "```python\n",
    "relu(-3)\n",
    "```\n",
    "Should return:\n",
    "```python\n",
    "0\n",
    "```\n",
    "Since $-3 < 0$, and\n",
    "```python\n",
    "relu(3)\n",
    "```\n",
    "Should return:\n",
    "```python\n",
    "3\n",
    "```\n",
    "Since $3 \\geq 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3:** Now, let's code our activation function for our output layer! Let's assume we are building a neural network for a regression problem. What activation function should we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer: Identity function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4:** Write a function called `output_activation` that takes in a value and passes it through the activation function you decided on in problem 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def output_activation(value):\n",
    "    return identity(value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5**: Put it together! We will use the functions we wrote above to build a simple neural network to predict the price of a home given home size and quality on a scale of 1-5. For simplicity, we will build a network with one hidden layer with only one node.\n",
    "\n",
    "Our general network architecture will look like this:\n",
    "\n",
    "![](./images/network_drawing.png)\n",
    "\n",
    "The simple data we will work with is given here:\n",
    "\n",
    "| square_feet | quality | price |\n",
    "| --- | --- | --- |\n",
    "| 1750 | 3 | \\$250,000 |\n",
    "| 2500 | 4 | \\$600,000 |\n",
    "| 4320 | 5 | \\$895,000 |\n",
    "| 1300 | 1 | \\$195,000 |\n",
    "\n",
    "We will normalize our data to ensure all values are between 0 and 1. This will help our neural network converge and ensure that we do not lose any weights during the fitting process (since any negative values will be set to 0 when they are passed through the ReLU function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put data from above into a numpy array for modeling\n",
    "\n",
    "housing_data = np.array([[1750, 3, 250_000],\n",
    "                        [2500, 4, 600_000],\n",
    "                        [4320, 5, 895_000],\n",
    "                        [1300, 1, 195_000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split housing data into X & y\n",
    "X = housing_data[:,[0,1]]\n",
    "y = housing_data[:,2]\n",
    "\n",
    "# Normalize each column by scaling all values into the range [0, 1]\n",
    "#   - Avoids negative values for ReLU, which would prevent weight updates\n",
    "#   - ptp: peak-to-peak, i.e. the range of each column (max - min)\n",
    "X_scaled = (X - X.min(axis=0)) / X.ptp(axis=0)\n",
    "\n",
    "# Normalize y identically\n",
    "#    - Ensures the weights do not have to be enormously adjusted with a huge learning rate\n",
    "y_scaled = (y - y.min()) / y.ptp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we will only use **one observation** of our data to start!\n",
    "\n",
    "| square_feet | quality | price |\n",
    "| --- | --- | --- |\n",
    "| 1750 | 3 | \\$250,000 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the first row of the dataset for simplicity\n",
    "\n",
    "X_scaled_simple = X_scaled[0,:]\n",
    "y_simple = y_scaled[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run this through our network!\n",
    "\n",
    "To do this, pass the `X_scaled_simple` data, `w1` and `w1` from the given weights vector, and `b1` from the given bias vector into your `update` function. Save the returned array as `sum1`.\n",
    "\n",
    "We will use randomly generated weights and bias to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random weights and biases\n",
    "# We need 3 weights and 2 biases\n",
    "# If you are unsure why this is, go back to\n",
    "# the picture of the network architecture above!\n",
    "\n",
    "# Weights & bias vectors (positive to avoid reLU zeroing out)\n",
    "weights = np.abs(np.random.normal(size=3))  # [w1, w2, w3]\n",
    "biases = np.abs(np.random.normal(size=2))   # [b1, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum1: [-3.37762128 -0.16958244 -4.54218733 -1.66005974  1.39632769  1.44723464\n",
      " -0.99895631 -0.05064637 -3.08402772 -0.49307531]\n"
     ]
    }
   ],
   "source": [
    "# Get sum1:\n",
    "# Answer:\n",
    "\n",
    "def update(X, weight1, bias1):\n",
    "    sum1 = np.dot(X, weight1) + bias1\n",
    "    return sum1\n",
    "\n",
    "# Example usage:\n",
    "np.random.seed(0)  # For reproducibility\n",
    "X_scaled_simple = np.random.randn(10, 2)  # Example data: 10 samples, 2 features each\n",
    "w1 = np.random.randn(2)  # Random weights for 2 features\n",
    "b1 = np.random.randn(1)  # Random bias\n",
    "\n",
    "sum1 = update(X_scaled_simple, w1, b1)\n",
    "print(\"sum1:\", sum1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6:** Apply your hidden layer activation function `relu` to your `sum1`. Save the result of this as `out1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out1: [0.         0.         0.         0.         1.39632769 1.44723464\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Get out1:\n",
    "# Answer:\n",
    "out1 = relu(sum1)\n",
    "print(\"out1:\", out1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 7**: We will now pass the output from the hidden layer to our output layer. To do this:\n",
    "1. Pass the `out1` result, `w3` from the given weights vector, and `b2` from the given bias vector through your `update` function. Save the result as `sum2`.\n",
    "2. Apply your output layer activation function `output_activation` to your `sum2`. Save the result of this as `out2`. This is our prediction of the price of that first home!\n",
    "\n",
    "*Hint: Your `update` function takes in an array instead of just a single number! You may have to convert `out1` to an array before running it through your `update` function!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out2: [1.53277921 1.53277921 1.53277921 1.53277921 1.27140922 1.26188026\n",
      " 1.53277921 1.53277921 1.53277921 1.53277921]\n"
     ]
    }
   ],
   "source": [
    "# Get sum2 and out2:\n",
    "# Answer:\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def output_activation(x):\n",
    "    return identity(x)\n",
    "\n",
    "# Example weights and biases\n",
    "w3 = np.random.randn(1)  # weight for output layer\n",
    "b2 = np.random.randn(1)  # bias for output layer\n",
    "\n",
    "# Assume out1 is already computed and is a numpy array\n",
    "# If out1 is not an array or needs to be reshaped:\n",
    "out1 = np.array(out1).reshape(-1, 1)  # Ensure out1 is a column vector if not already\n",
    "\n",
    "# Calculate sum2 using the update function\n",
    "sum2 = update(out1, w3, b2)\n",
    "\n",
    "# Apply the output activation function\n",
    "out2 = output_activation(sum2)\n",
    "\n",
    "print(\"out2:\", out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 8**: Put it together! Write a function called `forward` that goes through a complete forward pass (combines all of your steps from problems 5-7). This function should take in a row of the data, weights, and biases and:\n",
    "1. Calculates `sum1`: the result of passing the row of data, the weights, and the bias through your `update` function.\n",
    "2. Calculates `out1`: the result of applying your hidden layer activation function `relu` to your `sum1` result from step 1.\n",
    "3. Calculates `sum2`: the result of passing `out1`, the weights, and the bias through your `update` function.\n",
    "4. Calculates `out2`: the result of applying your output layer activation function `output_activation` to your `sum2` result from step 3.\n",
    "5. Returns `sum1`, `out1`, `sum2`, and `out2`!\n",
    "\n",
    "You have now written a function that completes one forward pass through our neural network! To check your work, you should get the same results from problems 5-7 here (assuming you are using the same randomly generated weights and biases vectors)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of forward passes: [(array([0.89710859]), array([[0.89710859]]), array([2.74559224]), array([2.74559224])), (array([1.93258739]), array([[1.93258739]]), array([3.75905468]), array([3.75905468])), (array([3.02285052]), array([[3.02285052]]), array([4.82613662]), array([4.82613662])), (array([5.11084252]), array([[5.11084252]]), array([6.8697337]), array([6.8697337])), (array([2.63314937]), array([[2.63314937]]), array([4.4447213]), array([4.4447213])), (array([3.42737869]), array([[3.42737869]]), array([5.2220637]), array([5.2220637])), (array([2.00425958]), array([[2.00425958]]), array([3.82920297]), array([3.82920297])), (array([-0.28737367]), array([[0.]]), array([1.86755799]), array([1.86755799])), (array([3.73982099]), array([[3.73982099]]), array([5.52786285]), array([5.52786285])), (array([1.83993393]), array([[1.83993393]]), array([3.66837121]), array([3.66837121]))]\n"
     ]
    }
   ],
   "source": [
    "# Answer:\n",
    "# Define or import necessary functions (update, relu, output_activation)\n",
    "def update(X, w, b):\n",
    "    return np.dot(X, w) + b\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def output_activation(x):\n",
    "    return identity(x)\n",
    "\n",
    "def forward(row, weights, biases):\n",
    "    # Unpack weights and biases\n",
    "    w1, w3 = weights\n",
    "    b1, b2 = biases\n",
    "\n",
    "    # Calculate sum1 (hidden layer pre-activation)\n",
    "    sum1 = update(row, w1, b1)\n",
    "    \n",
    "    # Calculate out1 (hidden layer post-activation)\n",
    "    out1 = relu(sum1)\n",
    "\n",
    "    # Ensure out1 is properly shaped for multiplication with w3\n",
    "    out1 = np.array(out1).reshape(-1, 1)  # Ensure out1 is a column vector\n",
    "\n",
    "    # Calculate sum2 (output layer pre-activation)\n",
    "    sum2 = update(out1, w3, b2)\n",
    "\n",
    "    # Calculate out2 (output layer post-activation)\n",
    "    out2 = output_activation(sum2)\n",
    "\n",
    "    return sum1, out1, sum2, out2\n",
    "\n",
    "# Setup weights and biases\n",
    "np.random.seed(0)\n",
    "weights = [np.random.randn(2), np.random.randn(1)]  # Correct number of weights for two layers\n",
    "biases = [np.random.randn(1), np.random.randn(1)]   # Biases for each layer\n",
    "\n",
    "# Example: Assuming X_scaled_simple has multiple samples\n",
    "X_scaled_simple = np.random.randn(10, 2)  # 10 samples, 2 features each\n",
    "\n",
    "# Process each row individually\n",
    "results = [forward(row, weights, biases) for row in X_scaled_simple]\n",
    "\n",
    "print(\"Results of forward passes:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.89710859,  1.93258739,  3.02285052,  5.11084252,  2.63314937,\n",
       "         3.42737869,  2.00425958, -0.28737367,  3.73982099,  1.83993393]),\n",
       " array([[0.89710859],\n",
       "        [1.93258739],\n",
       "        [3.02285052],\n",
       "        [5.11084252],\n",
       "        [2.63314937],\n",
       "        [3.42737869],\n",
       "        [2.00425958],\n",
       "        [0.        ],\n",
       "        [3.73982099],\n",
       "        [1.83993393]]),\n",
       " array([2.74559224, 3.75905468, 4.82613662, 6.8697337 , 4.4447213 ,\n",
       "        5.2220637 , 3.82920297, 1.86755799, 5.52786285, 3.66837121]),\n",
       " array([2.74559224, 3.75905468, 4.82613662, 6.8697337 , 4.4447213 ,\n",
       "        5.2220637 , 3.82920297, 1.86755799, 5.52786285, 3.66837121]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(X_scaled_simple, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 9:** Now, we need to quantify our loss. Since this is a regression problem, we will use the mean squared error (MSE) loss, which can be written as:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_{true}-y_{pred})^2 \n",
    "$$\n",
    "\n",
    "Write a function called `mse` that takes in two arrays, `y_true` and `y_pred` and returns the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.375\n"
     ]
    }
   ],
   "source": [
    "# Loss function:\n",
    "# Answer:\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    # Calculate the mean squared error between true and predicted values\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Example usage:\n",
    "y_true = np.array([3, -0.5, 2, 7])\n",
    "y_pred = np.array([2.5, 0.0, 2, 8])\n",
    "\n",
    "# Calculate MSE\n",
    "error = mse(y_true, y_pred)\n",
    "print(\"Mean Squared Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, find the mean squared error between your `out2` (y_pred) and `y_simple` (y_true). Since your function took in arrays and we are just working with one example for now, we will convert these values to arrays of length 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_simple and out2 to arrays to use in function:\n",
    "\n",
    "y_simple = np.array([y_simple])\n",
    "output = np.array([out2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.974084407148236"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y_simple, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is your model doing? Probably very bad! This is where the magic happens: backpropagation. Backpropagation is the process of using gradient descent to update our weights and biases in order to try to minimize the loss function (mean squared error in this case). Check out the gradient descent lesson if you need a reminder on how this works.\n",
    "\n",
    "We will be using the terms $w_1$, $h_1$, $w_3$, etc. for this next part. Here is a reminder of our network that labels all of the weights with these terms. As a simple example, we will work our way backwards through one portion of the network to update $w_1$ (through $o_1$ (predicted price), $w_3$, and $h_1$).\n",
    "\n",
    "![](./images/network_drawing.png)\n",
    "\n",
    "We can define our new weights (and biases) using gradient descent. Again, for simplicity we will just work through updating $w_1$:\n",
    "\n",
    "$$\n",
    "w_1 \\leftarrow w_1 - \\lambda \\frac{\\partial L}{\\partial w_1}\n",
    "$$\n",
    "\n",
    "Since we work backwards through our network with multiple variables, we calculate the derivative of the loss with respect to $w_1$ ($ \\frac{\\partial L}{\\partial w_1} $) using [partial derivatives](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial y_{pred}} * \\frac{\\partial y_{pred}}{\\partial h_1} * \\frac{\\partial h_1}{\\partial w_1}\n",
    "$$\n",
    "\n",
    "Where $L$ is the loss, $w_1$ is the first weight, $h_1$ is the output from the first hidden layer, and $y_{pred}$ is the predicted value of y.\n",
    "\n",
    "For those of you familiar with calculus, feel free to try to compute these partial derivatives on your own! For those of you not as familiar, I will give you each of these for our scenario. Remember, we are using MSE as our loss function and ReLU as our hidden layer activation function:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y_{pred}} = -2(y_{true}-y_{pred})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_{pred}}{\\partial h_1} = w_3*f'(w_3h_1+b_2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial w_1} = x_1*f'(w_1x_1+w_2x_2+b_1)\n",
    "$$\n",
    "\n",
    "Where $f'$ is the derivative of the ReLU function:\n",
    "\n",
    "$$\n",
    "f'(x) = \\left\\{\\begin{matrix}\n",
    "0 \\text{  for  } x<0\\\\\n",
    "1 \\text{  for  } x\\geq 0\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "These last two work because of the [chain rule](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/a/chain-rule-review).\n",
    "\n",
    "We now have all the pieces to update our weights and biases! We are technically using stochastic gradient descent to update our weights and biases here, since we only operate on one sample at a time. The general process for backpropagation will be:\n",
    "1. Select one sample from our dataset.\n",
    "2. Calculate the partial derivatives.\n",
    "3. Use the equation from gradient descent to update each weight and bias ($w_1 \\leftarrow w_1 - \\lambda \\frac{\\partial L}{\\partial w_1}$).\n",
    "4. Repeat for each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 10:** Write a function called `relu_deriv` that is the derivative of the ReLU function:\n",
    "\n",
    "$$\n",
    "f'(x) = \\left\\{\\begin{matrix}\n",
    "0 \\text{  for  } x<0\\\\\n",
    "1 \\text{  for  } x\\geq 0\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "This function should take in an input and return the derivative of the ReLU function at that input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of ReLU:\n",
    "# Answer:\n",
    "def relu_deriv(x):\n",
    "    # Compute the derivative of the ReLU function\n",
    "    return (x >= 0).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "We are now ready to create a neural network from scratch! \n",
    "\n",
    "Again, for simplicity all we will do right now is write code that updates $w_1$ to start. (In order to actually minimize our loss function, we will need to expand this to update $w_2$, $w_3$, $b_1$, and $b_2$ as well. We will do this later on.)\n",
    "\n",
    "![](./images/network_drawing.png)\n",
    "\n",
    "*Note*: this is definitely not going to find an optimal model since this is an extremely simplified example. Hopefully you have developed more of an appreciation for Keras!\n",
    "\n",
    "**Problem 11:** Write a function called `update_weights` that takes in your X data, y data, weights, biases, and a learning rate ($\\lambda$) in order to update the weights using gradient descent. This function should:\n",
    "1. Go through one forward pass of your network to generate a prediction (use your `forward` function here) for each row in your dataset.\n",
    "2. Goes through the process of backpropagation to find $\\frac{\\partial L}{\\partial w_1}$: $\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial y_{pred}} * \\frac{\\partial y_{pred}}{\\partial h_1} * \\frac{\\partial h_1}{\\partial w_1}$ (this code is given for you).\n",
    "3. Updates $w_1$ using gradient descent ($w_1 \\leftarrow w_1 - \\lambda \\frac{\\partial L}{\\partial w_1}$).\n",
    "4. Calculates the loss.\n",
    "5. Returns the loss.\n",
    "\n",
    "Comments and starter code is given to help you work through this problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(X, y_true, weights, biases, learning_rate):\n",
    "    y_preds = np.zeros_like(y_true)\n",
    "    for i, (row, y_actual) in enumerate(zip(X, y_true)):\n",
    "        # Ensure the correct number of weights and biases are passed to forward function\n",
    "        sum1, out1, sum2, out2 = forward(row, weights, biases)\n",
    "        y_preds[i] = out2\n",
    "\n",
    "        dl_dypred = 2 * (out2 - y_actual)\n",
    "        dypred_dh1 = weights[1] * relu_deriv(sum2)  # Assuming second set of weights for the output layer\n",
    "        dh1_dw1 = row[0] * relu_deriv(sum1)\n",
    "\n",
    "        # Gradient descent to update the first set of weights (input to hidden layer)\n",
    "        weights[0] -= learning_rate * dl_dypred * dypred_dh1 * dh1_dw1\n",
    "\n",
    "    # Calculate loss using MSE\n",
    "    loss = mse(y_true, y_preds)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.18677081803152"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_weights(X_scaled, y_scaled, weights, biases, learning_rate = 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You've updated $w_1$ using gradient descent and returned your loss! Now, lets put this all together to update $w_1$ in our network to actually train our model. Since this is challenging, this function is given for you. As long as you have successfully completed the problems up to this point, your neural network should work and we can actually train our model!\n",
    "\n",
    "The first function is called `plot_predictions` and allows us to visualize how our model is doing by plotting the loss via gradient descent and the difference between the true square footage and our model's predictions.\n",
    "\n",
    "The second function is called `train` and trains our model by taking in your X data, y data, a learning rate, and a certain number of epochs. For each epoch, the network:\n",
    "1. Goes through a forward pass using your `forward` function.\n",
    "2. Goes through a backward pass to calculate the loss using your `update_weights` function.\n",
    "3. Uses the `plot_predictions` function to visualize how our model is doing!\n",
    "4. Repeats steps 1-3 for the given number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(X, y_true, weights, biases):\n",
    "    # Loss scatter plot titles\n",
    "    plt.title('Loss via Gradient Descent')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    # Side-by-side prediction plots\n",
    "    fig, ax = plt.subplots(1, 2, sharey=True)\n",
    "    fig.suptitle('Actual (blue) vs. predicted (orange)')\n",
    "    ax[0].set_ylabel('price (normalized)')\n",
    "    ax[0].set_xlabel('square feet (normalized)')\n",
    "    ax[1].set_xlabel('quality (normalized)')\n",
    "    \n",
    "    # Display each predicted value\n",
    "    for i, (row, y_actual) in enumerate(zip(X, y_true)):\n",
    "        *_, y_pred = forward(row, weights, biases)\n",
    "        ax[0].scatter(row[0], y_pred, c='orange')\n",
    "        ax[0].scatter(row[0], y_actual, c='b')\n",
    "        ax[1].scatter(row[1], y_pred, c='orange')\n",
    "        ax[1].scatter(row[1], y_actual, c='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create code for training\n",
    "def train(X, y_true, learning_rate, epochs):\n",
    "    \"\"\" Initialize and train netork for given epochs.\"\"\"\n",
    "    \n",
    "    # Initialize random weights and biases:\n",
    "    weights = np.abs(np.random.normal(size=3))  # [w1, w2, w3]\n",
    "    biases = np.abs(np.random.normal(size=2))   # [b1, b2]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass + Backpropagation + Gradient Descent\n",
    "        loss = update_weights(X, y_true, weights, biases, learning_rate)\n",
    "      \n",
    "        #print(f'Loss at epoch {epoch}: {loss}')\n",
    "        plt.scatter(epoch, loss, c='b')\n",
    "    \n",
    "    print('Final loss:', loss)\n",
    "    plot_predictions(X, y_true, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_hidden_units' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Call the train function\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[51], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(X, y_true, learning_rate, epochs)\u001b[0m\n\u001b[0;32m      3\u001b[0m num_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Assuming a single output for regression\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize weights and biases with correct dimensions\u001b[39;00m\n\u001b[0;32m      6\u001b[0m weights \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m----> 7\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(num_features, \u001b[43mnum_hidden_units\u001b[49m),  \u001b[38;5;66;03m# Input to hidden layer\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(num_hidden_units, num_outputs)  \u001b[38;5;66;03m# Hidden to output layer\u001b[39;00m\n\u001b[0;32m      9\u001b[0m ]\n\u001b[0;32m     10\u001b[0m biases \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(num_hidden_units),  \u001b[38;5;66;03m# Hidden layer biases\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(num_outputs)  \u001b[38;5;66;03m# Output layer biases\u001b[39;00m\n\u001b[0;32m     13\u001b[0m ]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_hidden_units' is not defined"
     ]
    }
   ],
   "source": [
    "def train(X, y_true, learning_rate, epochs):\n",
    "    num_features = X.shape[1]\n",
    "    num_outputs = 1  # Assuming a single output for regression\n",
    "\n",
    "    # Initialize weights and biases with correct dimensions\n",
    "    weights = [\n",
    "        np.random.randn(num_features, num_hidden_units),  # Input to hidden layer\n",
    "        np.random.randn(num_hidden_units, num_outputs)  # Hidden to output layer\n",
    "    ]\n",
    "    biases = [\n",
    "        np.random.randn(num_hidden_units),  # Hidden layer biases\n",
    "        np.random.randn(num_outputs)  # Output layer biases\n",
    "    ]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = update_weights(X, y_true, weights, biases, learning_rate)\n",
    "        print(f'Loss at epoch {epoch}: {loss}')\n",
    "\n",
    "# Call the train function\n",
    "train(X_scaled, y_scaled, 0.02, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the `train` function on the `X_scaled` and `y_scaled` data! Your loss should be decreasing each epoch, but this will be a sub-optimal model since we are only updating $w_1$ for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_hidden_units' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m)\u001b[49m;\n",
      "Cell \u001b[1;32mIn[51], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(X, y_true, learning_rate, epochs)\u001b[0m\n\u001b[0;32m      3\u001b[0m num_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Assuming a single output for regression\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize weights and biases with correct dimensions\u001b[39;00m\n\u001b[0;32m      6\u001b[0m weights \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m----> 7\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(num_features, \u001b[43mnum_hidden_units\u001b[49m),  \u001b[38;5;66;03m# Input to hidden layer\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(num_hidden_units, num_outputs)  \u001b[38;5;66;03m# Hidden to output layer\u001b[39;00m\n\u001b[0;32m      9\u001b[0m ]\n\u001b[0;32m     10\u001b[0m biases \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(num_hidden_units),  \u001b[38;5;66;03m# Hidden layer biases\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(num_outputs)  \u001b[38;5;66;03m# Output layer biases\u001b[39;00m\n\u001b[0;32m     13\u001b[0m ]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_hidden_units' is not defined"
     ]
    }
   ],
   "source": [
    "train(X_scaled, y_scaled, 0.02, 150);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not satisfied with the performance of your model? Well that's because we were only updating one of the weights! What happens when we update **all** of our weights and biases using this process? The code below will do that for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update ALL weights and biases\n",
    "def update_weights(X, y_true, weights, biases, learning_rate):\n",
    "    \"\"\" Updates the weight values. Returns the resulting loss. \"\"\"\n",
    "    \n",
    "    # Initialize prediction array with 0's\n",
    "    y_preds = np.zeros_like(y_true)\n",
    "    \n",
    "    for i, (row,y_actual) in enumerate(zip(X, y_true)):\n",
    "        # Forward pass\n",
    "        sum1,out1, sum2,out2 = forward(row, weights, biases)\n",
    "        y_preds[i] = out2\n",
    "\n",
    "        # Backpropagation\n",
    "        dl_dypred = 2 * (out2 - y_actual)\n",
    "        dypred_dh1 = weights[2] * relu_deriv(sum2)\n",
    "\n",
    "        dh1_dw1 = row[0] * relu_deriv(sum1)\n",
    "        dh1_dw2 = row[1] * relu_deriv(sum1)\n",
    "        dh1_db1 = relu_deriv(sum1)      \n",
    "        \n",
    "        dypred_dw3 = out1 * relu_deriv(sum2)\n",
    "        dypred_db2 = relu_deriv(sum2)\n",
    "        \n",
    "        # Gradient descent - Update weights and biases\n",
    "        weights[0] -= learning_rate * dl_dypred * dypred_dh1 * dh1_dw1\n",
    "        weights[1] -= learning_rate * dl_dypred * dypred_dh1 * dh1_dw2\n",
    "        weights[2] -= learning_rate * dl_dypred * dypred_dw3\n",
    "\n",
    "        biases[0] -= learning_rate * dl_dypred * dypred_dh1 * dh1_db1\n",
    "        biases[1] -= learning_rate * dl_dypred * dypred_db2\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = mse(y_true, y_preds)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_hidden_units' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.004\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m;\n",
      "Cell \u001b[1;32mIn[51], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(X, y_true, learning_rate, epochs)\u001b[0m\n\u001b[0;32m      3\u001b[0m num_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Assuming a single output for regression\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize weights and biases with correct dimensions\u001b[39;00m\n\u001b[0;32m      6\u001b[0m weights \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m----> 7\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(num_features, \u001b[43mnum_hidden_units\u001b[49m),  \u001b[38;5;66;03m# Input to hidden layer\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(num_hidden_units, num_outputs)  \u001b[38;5;66;03m# Hidden to output layer\u001b[39;00m\n\u001b[0;32m      9\u001b[0m ]\n\u001b[0;32m     10\u001b[0m biases \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(num_hidden_units),  \u001b[38;5;66;03m# Hidden layer biases\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(num_outputs)  \u001b[38;5;66;03m# Output layer biases\u001b[39;00m\n\u001b[0;32m     13\u001b[0m ]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_hidden_units' is not defined"
     ]
    }
   ],
   "source": [
    "train(X_scaled, y_scaled, 0.004, 400);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You've created a *very* simple neural network using only numpy! Hopefully by now you have a better understanding of the math behind a neural network.\n",
    "\n",
    "*Credit: inspiration for this lab comes from [this blog post](https://victorzhou.com/blog/intro-to-neural-networks/).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
