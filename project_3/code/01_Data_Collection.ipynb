{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3 - Classifying Grab's Customer Feedback into Ride Hailing and Delivery\n",
    "\n",
    "> Authors: Irfan Muzafar, Ng Wei, Lim Zheng Gang\n",
    "---\n",
    "\n",
    "**Problem Statement:**  \n",
    "How can we distinguish between customer feedback related to Grab's ride-hailing and delivery fast and accurately? \n",
    "\n",
    "**Target Audience:**\n",
    "Grab Product Team\n",
    "\n",
    "**Summary:**\n",
    "- Product Manager is overwhelmed by vast user reviews; struggles to classify reviews between delivery and ride hailing.\n",
    "- Develop a NLP model to differentiate ride-hailing comments from delivery on Grab's Google Play Store.\n",
    "- Training data: Uber and Uber Eats subreddit.\n",
    "- Other data sources: App store reviews from Grab and Grab Driver applications as well as those from competitors:  \n",
    "  - foodpanda  \n",
    "  - Deliveroo  \n",
    "  - Gojek  \n",
    "\n",
    "There are a total of three notebooks for this project:  \n",
    " 1. `01_Data_Collection.ipynb`   \n",
    " 2. `02_Data_Cleaning_EDA.ipynb`   \n",
    " 3. `03_Modelling_Evaluation_Conclusion.ipynb`   \n",
    "\n",
    "---\n",
    "**This Notebook**\n",
    "- We will scrape the web to get the data we need to build as well as the app reviews from the various store.\n",
    "- Save them into CSV under the folder \"datasets\" for analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import libraries and authenticate Reddit access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports necessary libraries for web scraping and data manipulation in Python. Specifically, it includes `praw` for accessing Reddit data, `requests` for making HTTP requests, and `google_play_scraper` for scraping data from the Google Play Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google_play_scraper import app, Sort, reviews_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializes a Reddit instance using the PRAW. To use it, you'll need to replace the placeholders for `client_id`, `client_secret`, and `user_agent` with your own Reddit API credentials. You can obtain these credentials by creating a Reddit app on the Reddit Developer website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"gQ5g-Npw_eGanQVJz_IXew\",\n",
    "    client_secret=\"R4PEfOzcihdm6CkJtfnDBasc7-JHfQ\",\n",
    "    user_agent=\"Project 3 GA\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the credentials is valid. Expected response is `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(reddit.user.me())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Scraping for Reddit Data and save into CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterates through a list of subreddit names, which are 'uber' and 'UberEATS'. sends an HTTP GET request to the subreddit URL using the `requests` library to retrieve the webpage's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_names = ['uber', 'UberEATS']\n",
    "for subreddit_name in subreddit_names:\n",
    "    reddit_url = f'https://www.reddit.com/r/{subreddit_name}'\n",
    "    response = requests.get(reddit_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of posts in each sub-reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uber sub-reddit has 976 post\n",
      "Ubereat sub-reddit has 988 post\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subreddit_uber = reddit.subreddit(\"uber\")\n",
    "\n",
    "count_submission_uber = 0 \n",
    "for submission in subreddit_uber.new(limit = None):\n",
    "    count_submission_uber += 1\n",
    "\n",
    "subreddit_ubereats = reddit.subreddit(\"UberEATS\")\n",
    "\n",
    "count_submission_ubereats = 0 \n",
    "for submission in subreddit_ubereats.new(limit = None):\n",
    "    count_submission_ubereats += 1\n",
    "\n",
    "print(f\"Uber sub-reddit has {count_submission_uber} post\")\n",
    "print(f\"Ubereat sub-reddit has {count_submission_ubereats} post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape comments from the top 100 submissions of each subreddit. This approach aims to optimize processing time while ensuring the retrieval of the most relevant data. By limiting the scope to the top submissions, the script focuses on extracting high-engagement content, which can provide valuable insights for analysis or research purposes. Save them into `csv` at the end of the scraping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_ubereats saved to ../datasets/reddit_ubereats_data_test.csv\n"
     ]
    }
   ],
   "source": [
    "data_ubereats = []\n",
    "\n",
    "subreddit_ubereats = reddit.subreddit(\"UberEATS\")\n",
    "\n",
    "for submission in subreddit_ubereats.top(limit= 100):\n",
    "    data_ubereats.append({\n",
    "    \"submission/comment id\": submission.id,\n",
    "    \"submission or comment\": \"submission.title\",\n",
    "    \"body\": submission.title,\n",
    "    \"up_votes\": submission.ups,\n",
    "    })\n",
    "\n",
    "    data_ubereats.append({\n",
    "    \"submission/comment id\": submission.id,\n",
    "    \"submission or comment\": \"submission\",\n",
    "    \"body\": submission.selftext,\n",
    "    \"up_votes\": submission.ups,\n",
    "    })\n",
    "\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    for comment in submission.comments.list():\n",
    "        data_ubereats.append({\n",
    "            \"submission/comment id\": comment.id,\n",
    "            \"submission or comment\": \"comment\",\n",
    "            \"body\": comment.body,\n",
    "            \"up_votes\": comment.ups,\n",
    "        })\n",
    "\n",
    "df_ubereats = pd.DataFrame(data_ubereats)\n",
    "\n",
    "# Save the data_ubereatsFrame to a CSV file\n",
    "output_path = '../datasets/reddit_ubereats_data_final.csv'\n",
    "df_ubereats.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"data_ubereats saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_uber saved to ../datasets/reddit_uber_data_test.csv\n"
     ]
    }
   ],
   "source": [
    "# List to store data_uber\n",
    "data_uber = []\n",
    "\n",
    "subreddit_uber = reddit.subreddit(\"uber\")\n",
    "\n",
    "\n",
    "for submission in subreddit_uber.top(limit= 100):\n",
    "    data_uber.append({\n",
    "    \"submission/comment id\": submission.id,\n",
    "    \"submission or comment\": \"submission.title\",\n",
    "    \"body\": submission.title,\n",
    "    \"up_votes\": submission.ups,\n",
    "    })\n",
    "\n",
    "    data_uber.append({\n",
    "    \"submission/comment id\": submission.id,\n",
    "    \"submission or comment\": \"submission\",\n",
    "    \"body\": submission.selftext,\n",
    "    \"up_votes\": submission.ups,\n",
    "    })\n",
    "\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    for comment in submission.comments.list():\n",
    "        data_uber.append({\n",
    "            \"submission/comment id\": comment.id,\n",
    "            \"submission or comment\": \"comment\",\n",
    "            \"body\": comment.body,\n",
    "            \"up_votes\": comment.ups,\n",
    "        })\n",
    "\n",
    "\n",
    "# Convert data_uber to a data_uberFrame\n",
    "df_uber = pd.DataFrame(data_uber)\n",
    "\n",
    "# Save the data_uberFrame to a CSV file\n",
    "output_path = '../datasets/reddit_uber_data_final.csv'\n",
    "df_uber.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"data_uber saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Scrape app store data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis, we gather data from various app stores, including Grab and its competitors such as foodpanda, Deliveroo, and Gojek. By scraping app store data, we aim to collect comprehensive information about these platforms, including user reviews, ratings, and other relevant metadata. This data enables us to conduct a thorough analysis of the competitive landscape, identify trends, and gain insights into user preferences and experiences across different ride-hailing and food delivery services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../datasets/grab_data_playstore.csv\n"
     ]
    }
   ],
   "source": [
    "grab_reviews = reviews_all(\n",
    "    'com.grabtaxi.passenger',\n",
    "    sleep_milliseconds=0,\n",
    "    lang='en',\n",
    "    country='US',\n",
    "    sort=Sort.NEWEST,\n",
    ")\n",
    "\n",
    "# convert to dataframe\n",
    "df_grab_review = pd.DataFrame(np.array(grab_reviews),columns=['review'])\n",
    "df_grab_review = df_grab_review.join(pd.DataFrame(df_grab_review.pop('review').tolist()))\n",
    "\n",
    "output_path = '../datasets/grab_data_playstore.csv'\n",
    "df_grab_review.to_csv(output_path, index=False)\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../datasets/grab_driver_data_playstore.csv\n"
     ]
    }
   ],
   "source": [
    "grab_driver_reviews = reviews_all(\n",
    "    'com.grabtaxi.driver2',\n",
    "    sleep_milliseconds=0,\n",
    "    lang='en',\n",
    "    country='US',\n",
    "    sort=Sort.NEWEST,\n",
    ")\n",
    "\n",
    "# convert to dataframe\n",
    "df_grab_driver_reviews = pd.DataFrame(np.array(grab_driver_reviews),columns=['review'])\n",
    "df_grab_driver_reviews = df_grab_driver_reviews.join(pd.DataFrame(df_grab_driver_reviews.pop('review').tolist()))\n",
    "\n",
    "output_path = '../datasets/grab_driver_data_playstore.csv'\n",
    "df_grab_driver_reviews.to_csv(output_path, index=False)\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gojek "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../datasets/gojek_data_playstore.csv\n"
     ]
    }
   ],
   "source": [
    "gojek_reviews = reviews_all(\n",
    "    'com.gojek.app',\n",
    "    sleep_milliseconds=0,\n",
    "    lang='en',\n",
    "    country='US',\n",
    "    sort=Sort.NEWEST,\n",
    ")\n",
    "\n",
    "# convert to dataframe\n",
    "df_gojek_review = pd.DataFrame(np.array(gojek_reviews),columns=['review'])\n",
    "df_gojek_review = df_gojek_review.join(pd.DataFrame(df_gojek_review.pop('review').tolist()))\n",
    "\n",
    "output_path = '../datasets/gojek_data_playstore.csv'\n",
    "df_gojek_review.to_csv(output_path, index=False)\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../datasets/gojek_driver_data_playstore.csv\n"
     ]
    }
   ],
   "source": [
    "gojek_driver_reviews = reviews_all(\n",
    "    'com.gojek.partner',\n",
    "    sleep_milliseconds=0,\n",
    "    lang='en',\n",
    "    country='US',\n",
    "    sort=Sort.NEWEST,\n",
    ")\n",
    "\n",
    "# convert to dataframe\n",
    "df_gojek_driver_reviews = pd.DataFrame(np.array(gojek_driver_reviews),columns=['review'])\n",
    "df_gojek_driver_reviews = df_gojek_driver_reviews.join(pd.DataFrame(df_gojek_driver_reviews.pop('review').tolist()))\n",
    "\n",
    "output_path = '../datasets/gojek_driver_data_playstore.csv'\n",
    "df_gojek_driver_reviews.to_csv(output_path, index=False)\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deliveroo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../datasets/droo_driver_data_playstore.csv\n"
     ]
    }
   ],
   "source": [
    "droo_driver_reviews = reviews_all(\n",
    "    'com.deliveroo.driverapp',\n",
    "    sleep_milliseconds=0,\n",
    "    lang='en',\n",
    "    country='US',\n",
    "    sort=Sort.NEWEST,\n",
    ")\n",
    "\n",
    "# convert to dataframe\n",
    "df_droo_driver_reviews = pd.DataFrame(np.array(droo_driver_reviews),columns=['review'])\n",
    "df_droo_driver_reviews = df_droo_driver_reviews.join(pd.DataFrame(df_droo_driver_reviews.pop('review').tolist()))\n",
    "\n",
    "output_path = '../datasets/droo_driver_data_playstore.csv'\n",
    "df_droo_driver_reviews.to_csv(output_path, index=False)\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../datasets/droo_data_playstore.csv\n"
     ]
    }
   ],
   "source": [
    "droo_reviews = reviews_all(\n",
    "    'com.deliveroo.orderapp',\n",
    "    sleep_milliseconds=0,\n",
    "    lang='en',\n",
    "    country='US',\n",
    "    sort=Sort.NEWEST,\n",
    ")\n",
    "\n",
    "# convert to dataframe\n",
    "df_droo_reviews = pd.DataFrame(np.array(droo_reviews),columns=['review'])\n",
    "df_droo_reviews = df_droo_reviews.join(pd.DataFrame(df_droo_reviews.pop('review').tolist()))\n",
    "\n",
    "output_path = '../datasets/droo_data_playstore.csv'\n",
    "df_droo_reviews.to_csv(output_path, index=False)\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### foodpanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../datasets/fp_data_playstore.csv\n"
     ]
    }
   ],
   "source": [
    "fp_reviews = reviews_all(\n",
    "    'com.global.foodpanda.android',\n",
    "    sleep_milliseconds=0,\n",
    "    lang='en',\n",
    "    country='US',\n",
    "    sort=Sort.NEWEST,\n",
    ")\n",
    "\n",
    "# convert to dataframe\n",
    "df_fp_reviews = pd.DataFrame(np.array(fp_reviews),columns=['review'])\n",
    "df_fp_reviews = df_fp_reviews.join(pd.DataFrame(df_fp_reviews.pop('review').tolist()))\n",
    "\n",
    "output_path = '../datasets/fp_data_playstore.csv'\n",
    "df_fp_reviews.to_csv(output_path, index=False)\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../datasets/fp_driver_data_playstore.csv\n"
     ]
    }
   ],
   "source": [
    "fp_driver_reviews = reviews_all(\n",
    "    'com.logistics.rider.foodpanda',\n",
    "    sleep_milliseconds=0,\n",
    "    lang='en',\n",
    "    country='US',\n",
    "    sort=Sort.NEWEST,\n",
    ")\n",
    "\n",
    "# convert to dataframe\n",
    "df_fp_driver_reviews = pd.DataFrame(np.array(fp_driver_reviews),columns=['review'])\n",
    "df_fp_driver_reviews = df_fp_driver_reviews.join(pd.DataFrame(df_fp_driver_reviews.pop('review').tolist()))\n",
    "\n",
    "output_path = '../datasets/fp_driver_data_playstore.csv'\n",
    "df_fp_driver_reviews.to_csv(output_path, index=False)\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Expected Output file\n",
    "At the end of this notebook, we expect to have the following files in our dataset folder, ready for analysis:\n",
    "\n",
    "- `reddit_uber_data_final.csv`\n",
    "- `reddit_ubereats_data_final.csv`\n",
    "- `grab_data_playstore.csv`\n",
    "- `grab_driver_data_playstore.csv`\n",
    "- `fp_data_playstore.csv`\n",
    "- `fp_driver_data_playstore.csv`\n",
    "- `droo_data_playstore.csv`\n",
    "- `droo_driver_data_playstore.csv`\n",
    "- `gojek_data_playstore.csv`\n",
    "- `gojek_driver_data_playstore.csv`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
